# -*- coding: utf-8 -*-
"""02_Classical_ML_Models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HBQw3UELxdjfEbMwiCFBcakw--BMBtJH
"""

# Commented out IPython magic to ensure Python compatibility.
"""
House Price Prediction - Classical ML Models Implementation
Student Name: Ameya Joshi
Student ID: 801486822
Course: Introduction to Machine Learning
Date: 12-02-2025

This notebook implements three machine learning models learned in class:
1. Linear Regression (from Module 3)
2. Ridge Regression (from Module 4 - Regularization)
3. Neural Network for Regression (from Module 8)

Using the preprocessed data from Notebook 1.
"""

# Importing stuff we need
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.linear_model import LinearRegression, Ridge
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import warnings
warnings.filterwarnings('ignore')
import pickle
import time

# Setting random seeds for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# Making plots look nicer
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")
# %matplotlib inline

print("=" * 70)
print("HOUSE PRICE PREDICTION - CLASS MODELS (IMPROVED DATA)")
print("=" * 70)
print("Using the better preprocessed data from Notebook 1\n")

# 1. LOADING THE IMPROVED DATA
print("\n1. Loading the improved preprocessed data...")

try:
    # Loading the data we saved in improved Notebook 1
    with open('processed_data_improved.pkl', 'rb') as file:
        X_train_improved, X_test_improved, y_train_improved, y_test_improved = pickle.load(file)

    print("✓ Successfully loaded improved data from processed_data_improved.pkl")
    print(f"  Training data shape: {X_train_improved.shape}")
    print(f"  Testing data shape: {X_test_improved.shape}")
    print(f"  Number of features: {X_train_improved.shape[1]}")

    # Loading feature names if available
    try:
        with open('feature_names.pkl', 'rb') as f:
            feature_names_list = pickle.load(f)
        print(f"  Feature names loaded: {len(feature_names_list)} features")
    except:
        print("  Feature names file not found, using generic names")
        feature_names_list = [f"Feature_{i}" for i in range(X_train_improved.shape[1])]

except FileNotFoundError:
    print("✗ Oops! Couldn't find processed_data_improved.pkl")
    print("  Please run the improved Notebook 1 first.")
    raise

print(f"\nSample target values (first 5):")
print(y_train_improved.head() if hasattr(y_train_improved, 'head') else y_train_improved[:5])

# 2. FEATURE SCALING
print("\n2. Scaling the features for better model performance...")

# Creating a scaler
feature_scaler = StandardScaler()

# Scaling the training data and then the testing data with the same scaler
X_train_scaled_imp = feature_scaler.fit_transform(X_train_improved)
X_test_scaled_imp = feature_scaler.transform(X_test_improved)

print("✓ Features scaled using StandardScaler")
print("  Check - mean should be near 0, std near 1:")
print(f"    Mean of first 3 features: {X_train_scaled_imp[:, :3].mean(axis=0)}")
print(f"    Std of first 3 features: {X_train_scaled_imp[:, :3].std(axis=0)}")

# Saving the scaler for later
with open('improved_scaler.pkl', 'wb') as file:
    pickle.dump(feature_scaler, file)
print("✓ Scaler saved to 'improved_scaler.pkl'")

# 3. MODEL 1: LINEAR REGRESSION
print("\n" + "=" * 60)
print("MODEL 1: LINEAR REGRESSION")
print("=" * 60)
print("Basic linear model we learned in Module 3")

# Starting timer
linear_start_time = time.time()

print("\nTraining Linear Regression model...")
linear_model = LinearRegression()
linear_model.fit(X_train_scaled_imp, y_train_improved)

# Making predictions
print("Making predictions on test data...")
y_pred_linear = linear_model.predict(X_test_scaled_imp)

# Calculating performance metrics
mse_linear = mean_squared_error(y_test_improved, y_pred_linear)
mae_linear = mean_absolute_error(y_test_improved, y_pred_linear)
r2_linear = r2_score(y_test_improved, y_pred_linear)

# Cross-validation to check if results are stable
cv_scores_linear = cross_val_score(linear_model, X_train_scaled_imp, y_train_improved,
                                  cv=5, scoring='neg_mean_squared_error')
cv_mse_linear = -cv_scores_linear.mean()

linear_end_time = time.time()

print(f"\n=== LINEAR REGRESSION RESULTS ===")
print(f"Training time: {linear_end_time - linear_start_time:.2f} seconds")
print(f"Mean Squared Error (MSE): {mse_linear:,.2f}")
print(f"Mean Absolute Error (MAE): ${mae_linear:,.2f}")
print(f"R-squared Score: {r2_linear:.4f}")
print(f"Cross-Validation MSE: {cv_mse_linear:,.2f}")

# Looking at the most important features
print("\nTop 10 most important features for Linear Regression:")
if hasattr(linear_model, 'coef_'):
    coef_df = pd.DataFrame({
        'Feature': feature_names_list,
        'Coefficient': linear_model.coef_
    })
    # Taking absolute value to see importance
    coef_df['Abs_Coefficient'] = np.abs(coef_df['Coefficient'])
    top_features = coef_df.sort_values('Abs_Coefficient', ascending=False).head(10)

    print("\nFeature coefficients (positive means higher price):")
    for idx, row in top_features.iterrows():
        direction = "increases" if row['Coefficient'] > 0 else "decreases"
        print(f"  {row['Feature'][:30]:30} : {row['Coefficient']:8.2f} (price {direction})")

# 4. MODEL 2: RIDGE REGRESSION
print("\n" + "=" * 60)
print("MODEL 2: RIDGE REGRESSION")
print("=" * 60)
print("Regularized linear model from Module 4 (prevents overfitting)")

ridge_start_time = time.time()

# Trying different regularization strengths
alpha_options = [0.001, 0.01, 0.1, 1, 10, 100]
ridge_performance = []

print("\nTesting different alpha values (regularization strength):")
for alpha_val in alpha_options:
    temp_ridge = Ridge(alpha=alpha_val, random_state=42)
    temp_ridge.fit(X_train_scaled_imp, y_train_improved)
    temp_pred = temp_ridge.predict(X_test_scaled_imp)
    temp_mse = mean_squared_error(y_test_improved, temp_pred)
    ridge_performance.append((alpha_val, temp_mse))
    print(f"  alpha = {alpha_val:8}: MSE = {temp_mse:,.2f}")

# Finding the best alpha
best_alpha_value, best_ridge_mse = min(ridge_performance, key=lambda x: x[1])
print(f"\nBest alpha value: {best_alpha_value} (MSE: {best_ridge_mse:,.2f})")

# Training final model with best alpha
print("\nTraining final Ridge Regression model...")
ridge_final_model = Ridge(alpha=best_alpha_value, random_state=42)
ridge_final_model.fit(X_train_scaled_imp, y_train_improved)

# Making predictions
y_pred_ridge = ridge_final_model.predict(X_test_scaled_imp)

# Calculating metrics
mse_ridge = mean_squared_error(y_test_improved, y_pred_ridge)
mae_ridge = mean_absolute_error(y_test_improved, y_pred_ridge)
r2_ridge = r2_score(y_test_improved, y_pred_ridge)

ridge_end_time = time.time()

print(f"\n=== RIDGE REGRESSION RESULTS ===")
print(f"Training time: {ridge_end_time - ridge_start_time:.2f} seconds")
print(f"Mean Squared Error (MSE): {mse_ridge:,.2f}")
print(f"Mean Absolute Error (MAE): ${mae_ridge:,.2f}")
print(f"R-squared Score: {r2_ridge:.4f}")

# Comparing with linear regression
print(f"\nComparison with Linear Regression:")
if mse_linear > 0: #divind by 0 was creating issues hence now checking if the value is greater than 0
    mse_improvement = (mse_linear - mse_ridge) / mse_linear * 100
    print(f"  MSE Improvement: {mse_improvement:.1f}%")
print(f"  R² Improvement: {r2_ridge - r2_linear:.4f} ({(r2_ridge - r2_linear)*100:.2f} percentage points)")

# 5. MODEL 3: NEURAL NETWORK
print("\n" + "=" * 60)
print("MODEL 3: NEURAL NETWORK FOR REGRESSION")
print("=" * 60)
print("Neural network approach from Module 8 (more complex model)")

nn_start_time = time.time()

# Normalizing target variable for neural network
y_mean_value = y_train_improved.mean()
y_std_value = y_train_improved.std()
y_train_normalized = (y_train_improved - y_mean_value) / y_std_value
y_test_normalized = (y_test_improved - y_mean_value) / y_std_value

print("\nBuilding the neural network architecture...")
print("(Similar to what we did in Module 8 lab)")

# Simple feedforward neural network
neural_net = keras.Sequential([
    layers.Input(shape=(X_train_scaled_imp.shape[1],)),
    layers.Dense(128, activation='relu', name='first_hidden_layer'),
    layers.Dropout(0.3),  # Helps prevent overfitting
    layers.Dense(64, activation='relu', name='second_hidden_layer'),
    layers.Dropout(0.2),
    layers.Dense(32, activation='relu', name='third_hidden_layer'),
    layers.Dense(1, name='output_layer')  # No activation for regression
])

# Compiling the model
neural_net.compile(
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    loss='mse',  # Mean squared error for regression
    metrics=['mae']
)

print("\nNeural network architecture summary:")
neural_net.summary()

# Setting up callbacks for better training
training_callbacks = [
    keras.callbacks.EarlyStopping(
        patience=20,  # Stopping if no improvement for 20 epochs
        restore_best_weights=True  # Keeping the best weights found
    ),
    keras.callbacks.ReduceLROnPlateau(
        factor=0.5,  # Reduced learning rate by half
        patience=10
    )
]

# Training the neural network
print("\nTraining neural network...")
print("(This might take a couple of minutes)")
training_history = neural_net.fit(
    X_train_scaled_imp, y_train_normalized,
    validation_split=0.2,  # Using 20% of training for validation
    epochs=150,  # Maximum epochs
    batch_size=32,
    callbacks=training_callbacks,
    verbose=1  # Shows progress
)

# Making predictions (need to convert back from normalized scale)
y_pred_normalized = neural_net.predict(X_test_scaled_imp).flatten()
y_pred_nn = y_pred_normalized * y_std_value + y_mean_value

# Calculating metrics
mse_nn = mean_squared_error(y_test_improved, y_pred_nn)
mae_nn = mean_absolute_error(y_test_improved, y_pred_nn)
r2_nn = r2_score(y_test_improved, y_pred_nn)

nn_end_time = time.time()

print(f"\n=== NEURAL NETWORK RESULTS ===")
print(f"Training time: {nn_end_time - nn_start_time:.2f} seconds")
print(f"Mean Squared Error (MSE): {mse_nn:,.2f}")
print(f"Mean Absolute Error (MAE): ${mae_nn:,.2f}")
print(f"R-squared Score: {r2_nn:.4f}")

# Plotting training history
print("\nPlotting training progress...")
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Loss plot
ax1.plot(training_history.history['loss'], label='Training Loss', linewidth=2)
ax1.plot(training_history.history['val_loss'], label='Validation Loss', linewidth=2)
ax1.set_title('Neural Network Loss During Training', fontsize=14, fontweight='bold')
ax1.set_xlabel('Epoch')
ax1.set_ylabel('Loss (MSE)')
ax1.legend()
ax1.grid(True, alpha=0.3)

# MAE plot
ax2.plot(training_history.history['mae'], label='Training MAE', linewidth=2)
ax2.plot(training_history.history['val_mae'], label='Validation MAE', linewidth=2)
ax2.set_title('Neural Network MAE During Training', fontsize=14, fontweight='bold')
ax2.set_xlabel('Epoch')
ax2.set_ylabel('MAE')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('nn_training_progress.png', dpi=300, bbox_inches='tight')
plt.show()

# 6. COMPARING ALL THREE MODELS
print("\n" + "=" * 70)
print("COMPARING ALL THREE MODELS")
print("=" * 70)

# Creating comparison table
model_comparison = {
    'Model': ['Linear Regression', 'Ridge Regression', 'Neural Network'],
    'MSE': [mse_linear, mse_ridge, mse_nn],
    'MAE': [mae_linear, mae_ridge, mae_nn],
    'R² Score': [r2_linear, r2_ridge, r2_nn],
    'Training Time (s)': [linear_end_time-linear_start_time,
                         ridge_end_time-ridge_start_time,
                         nn_end_time-nn_start_time]
}

comparison_table = pd.DataFrame(model_comparison)
print("\nModel Performance Comparison:")
print(comparison_table.to_string(index=False))

# Visual comparison
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# MSE comparison
axes[0, 0].bar(comparison_table['Model'], comparison_table['MSE'],
               color=['blue', 'orange', 'green'])
axes[0, 0].set_title('Mean Squared Error (Lower is Better)', fontsize=12, fontweight='bold')
axes[0, 0].set_ylabel('MSE')
axes[0, 0].tick_params(axis='x', rotation=15)

# MAE comparison
axes[0, 1].bar(comparison_table['Model'], comparison_table['MAE'],
               color=['blue', 'orange', 'green'])
axes[0, 1].set_title('Mean Absolute Error (Lower is Better)', fontsize=12, fontweight='bold')
axes[0, 1].set_ylabel('MAE ($)')
axes[0, 1].tick_params(axis='x', rotation=15)

# R² comparison
axes[1, 0].bar(comparison_table['Model'], comparison_table['R² Score'],
               color=['blue', 'orange', 'green'])
axes[1, 0].set_title('R-squared Score (Higher is Better)', fontsize=12, fontweight='bold')
axes[1, 0].set_ylabel('R² Score')
axes[1, 0].set_ylim([0, 1])
axes[1, 0].tick_params(axis='x', rotation=15)

# Training time comparison
axes[1, 1].bar(comparison_table['Model'], comparison_table['Training Time (s)'],
               color=['blue', 'orange', 'green'])
axes[1, 1].set_title('Training Time (Seconds)', fontsize=12, fontweight='bold')
axes[1, 1].set_ylabel('Time (s)')
axes[1, 1].tick_params(axis='x', rotation=15)

plt.suptitle('Model Performance Comparison (Improved Data)', fontsize=16, fontweight='bold', y=1.02)
plt.tight_layout()
plt.savefig('model_comparison_improved.png', dpi=300, bbox_inches='tight')
plt.show()

# 7. PREDICTION VISUALIZATION
print("\n" + "=" * 60)
print("PREDICTION VISUALIZATION")
print("=" * 60)

# Taking a random sample of 15 houses for clear visualization
np.random.seed(42)
sample_indices = np.random.choice(len(y_test_improved), size=15, replace=False)
sample_indices = np.sort(sample_indices)

plt.figure(figsize=(14, 6))

# Actual values
actual_prices = y_test_improved.iloc[sample_indices].values if hasattr(y_test_improved, 'iloc') else y_test_improved[sample_indices]
x_positions = np.arange(len(sample_indices))

# Plotting
plt.plot(x_positions, actual_prices, 'ko-', linewidth=3, markersize=8, label='Actual Prices')
plt.plot(x_positions, y_pred_linear[sample_indices], 'bs--', linewidth=2, markersize=6, label='Linear Regression')
plt.plot(x_positions, y_pred_nn[sample_indices], 'g^-.', linewidth=2, markersize=6, label='Neural Network')

plt.xlabel('Sample House Index', fontsize=12)
plt.ylabel('Sale Price ($)', fontsize=12)
plt.title('Actual vs Predicted Prices (Sample of 15 Houses)', fontsize=14, fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)
plt.xticks(x_positions)

plt.tight_layout()
plt.savefig('predictions_comparison_improved.png', dpi=300, bbox_inches='tight')
plt.show()

print("\nIn the plot above:")
print("- Black line with circles: Actual sale prices")
print("- Blue squares with dashed line: Linear Regression predictions")
print("- Green triangles with dash-dot line: Neural Network predictions")
print("\nCloser to black line = better predictions!")

# 8. SAVING MODELS FOR LATER which will will be used in next notebook
print("\n" + "=" * 60)
print("SAVING MODELS FOR RESEARCH PAPER COMPARISON")
print("=" * 60)

# Saving all models
models_dict = {
    'linear_model': linear_model,
    'ridge_model': ridge_final_model,
    'neural_network': neural_net,
    'feature_scaler': feature_scaler,
    'target_stats': {'mean': y_mean_value, 'std': y_std_value}
}

with open('improved_class_models.pkl', 'wb') as file:
    pickle.dump(models_dict, file)

print("✓ All models saved to 'improved_class_models.pkl'")
print("  These will be used to compare with research paper methods in Notebook 3.")

# 9. SUMMARY AND OBSERVATIONS
print("\n" + "=" * 70)
print("SUMMARY AND OBSERVATIONS")
print("=" * 70)

print("\n1. Model Performance Ranking (Best to Worst):")
ranking = comparison_table.sort_values('R² Score', ascending=False)
for i, (_, row) in enumerate(ranking.iterrows(), 1):
    print(f"   {i}. {row['Model']}: R² = {row['R² Score']:.4f}, MSE = {row['MSE']:,.0f}")

print("\n2. Key Findings:")
print(f"   - Best model: {ranking.iloc[0]['Model']} with R² = {ranking.iloc[0]['R² Score']:.4f}")
print(f"   - Neural Network has {((ranking.iloc[0]['R² Score'] - ranking.iloc[-1]['R² Score'])/ranking.iloc[-1]['R² Score']*100):.1f}% better R² than worst model")
print(f"   - Ridge Regression improved over Linear Regression by {((r2_ridge - r2_linear)*100):.2f} percentage points")

print("\n3. Impact of Improved Preprocessing:")
print("   - Better handling of 'NA' values as 'None'")
print("   - More meaningful features created (HasBasement, HasGarage, etc.)")
print("   - Proper ordinal encoding based on data description")
print("   - Should lead to better model performance overall")

print("\n4. Next Steps for Research Paper Comparison:")
print("   - Implementing paper methods in Notebook 3")
print("\nModels trained and evaluated successfully.")