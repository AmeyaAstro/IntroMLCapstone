# -*- coding: utf-8 -*-
"""01_Data_Preprocessing_and_EDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Dss6wI9vH5-vo7B6L4_9AoGqUcBN7ydu
"""

# Commented out IPython magic to ensure Python compatibility.
"""
House Price Prediction - Data Preprocessing and EDA
Student Name: Ameya Joshi
Student ID: 801486822
Course: Introduction to Machine Learning
Date: 12-02-2025

This notebook handles data loading, cleaning, exploratory data analysis,
and feature engineering for the house price prediction.

Note: Using data_description.txt from Kaggle to properly understand features!
"""

# Importing necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")
# %matplotlib inline

print("=" * 70)
print("HOUSE PRICE PREDICTION - IMPROVED DATA PREPROCESSING")
print("=" * 70)
print("Using data_description.txt for informed preprocessing\n")

# 1. loading the data
print("\n1. Loading dataset and understanding features...")

# Loading the training data
train_df = pd.read_csv('train.csv')
print(f"✓ Training dataset loaded: {train_df.shape}")

# Loading test data
test_df = pd.read_csv('test.csv')
print(f"✓ Test dataset loaded: {test_df.shape}")
print("  Note: Test dataset doesn't have SalePrice (Kaggle competition)")
has_test_data = True


print("\nFirst 3 rows of training data:")
print(train_df.head(3))

# 2. UNDERSTAANDING THE DATA USING data_description.txt
print("\n" + "=" * 50)
print("2. Understanding Features from data_description.txt")
print("=" * 50)

print("\nKey insights from data_description.txt:")
print("- Many 'NA' values are NOT missing data but mean 'None' or 'Not Present'")
print("- Examples: Alley 'NA' = 'No alley access'")
print("-           PoolQC 'NA' = 'No Pool'")
print("-           Fence 'NA' = 'No Fence'")
print("\nWe need to handle these properly!")

features_with_meaningful_na = [
    'Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure',
    'BsmtFinType1', 'BsmtFinType2', 'FireplaceQu',
    'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',
    'PoolQC', 'Fence', 'MiscFeature'
]

print(f"\nFeatures where 'NA' has meaning: {len(features_with_meaningful_na)} features")
for feat in features_with_meaningful_na[:5]:
    print(f"  - {feat}")

# 3. EXPLORING THE DATA
print("\n" + "=" * 50)
print("3. Exploratory Data Analysis (Before Cleaning)")
print("=" * 50)

# Checking missing values before eda
print("\nMissing values (BEFORE cleaning):")
missing_before = train_df.isnull().sum()
missing_percent = (missing_before / len(train_df)) * 100
missing_df = pd.DataFrame({
    'Missing_Count': missing_before,
    'Missing_Percent': missing_percent
}).sort_values('Missing_Percent', ascending=False)

print(missing_df.head(15))

# Visualizing missing values
plt.figure(figsize=(15, 6))
sns.heatmap(train_df.isnull(), cbar=False, cmap='viridis', yticklabels=False)
plt.title('Missing Values Before Cleaning', fontsize=16, fontweight='bold')
plt.xlabel('Features', fontsize=12)
plt.tight_layout()
plt.savefig('missing_before_cleaning.png', dpi=300, bbox_inches='tight')
plt.show()

# 4. MISSINNG VALUE HANDLING
print("\n" + "=" * 50)
print("4. Intelligent Missing Value Handling")
print("=" * 50)
print("Based on data_description.txt understanding")

# Creating a copy for preprocessing that will be used in the notebook '02_Classical_ML_Models.ipynb'
df_clean = train_df.copy()

print("\nStep 1: Handling 'NA' values that actually mean 'None' or 'Not Present'")
# Replacing NaN with 'None' for features where NA has meaning
for feature in features_with_meaningful_na:
    if feature in df_clean.columns:
        na_count = df_clean[feature].isnull().sum()
        df_clean[feature].fillna('None', inplace=True)
        print(f"  - {feature}: {na_count} 'NA' → 'None'")

print("\nStep 2: Handling truly missing data")

# LotFrontage: Linear feet of street connected to property
# Filling with median of neighborhood (makes sense similar houses in same area)
print("\nHandling LotFrontage (street frontage):")
if 'LotFrontage' in df_clean.columns and 'Neighborhood' in df_clean.columns:
    # Grouping by neighborhood and filling with median
    df_clean['LotFrontage'] = df_clean.groupby('Neighborhood')['LotFrontage'].transform(
        lambda x: x.fillna(x.median())
    )
    print(f"  - Filled with neighborhood median")

# MasVnrType and MasVnrArea
print("\nHandling Masonry Veneer:")
if 'MasVnrType' in df_clean.columns:
    df_clean['MasVnrType'].fillna('None', inplace=True)
    print(f"  - MasVnrType: 'None' (no masonry veneer)")

if 'MasVnrArea' in df_clean.columns:
    df_clean['MasVnrArea'].fillna(0, inplace=True)
    print(f"  - MasVnrArea: 0 (no masonry veneer area)")

# Electrical
if 'Electrical' in df_clean.columns:
    df_clean['Electrical'].fillna(df_clean['Electrical'].mode()[0], inplace=True)
    print(f"  - Electrical: Filled with mode")
    #(only 1 missing value)

# GarageYrBlt (Year garage built)
if 'GarageYrBlt' in df_clean.columns:
    # If no garage, set to year house was built
    df_clean['GarageYrBlt'] = df_clean.apply(
        lambda row: row['YearBuilt'] if pd.isnull(row['GarageYrBlt']) and row['GarageType'] == 'None'
        else (row['YearBuilt'] if pd.isnull(row['GarageYrBlt']) else row['GarageYrBlt']),
        axis=1
    )
    print(f"  - GarageYrBlt: Set to YearBuilt for houses with no garage")

# Checking remaining missing values
remaining_missing = df_clean.isnull().sum().sum()
print(f"\n✓ Missing values after intelligent handling: {remaining_missing}")
if remaining_missing > 0:
    print("Remaining columns with missing values:")
    for col in df_clean.columns[df_clean.isnull().any()]:
        print(f"  - {col}: {df_clean[col].isnull().sum()} missing")

# 5. FEATURE ENGINNEERING
print("\n" + "=" * 50)
print("5. Smart Feature Enginneering")
print("=" * 50)
print("Creating new features based on housing domain knowledge")

# Total square footage
df_clean['TotalSF'] = df_clean['TotalBsmtSF'] + df_clean['1stFlrSF'] + df_clean['2ndFlrSF']

# Total bathrooms (full baths count as 1, half baths as 0.5)
df_clean['TotalBath'] = (df_clean['FullBath'] +
                        0.5 * df_clean['HalfBath'] +
                        df_clean['BsmtFullBath'] +
                        0.5 * df_clean['BsmtHalfBath'])

# Total porch area
df_clean['TotalPorchSF'] = (df_clean['OpenPorchSF'] +
                           df_clean['EnclosedPorch'] +
                           df_clean['3SsnPorch'] +
                           df_clean['ScreenPorch'])

# Age features
df_clean['HouseAge'] = df_clean['YrSold'] - df_clean['YearBuilt']
df_clean['RemodelAge'] = df_clean['YrSold'] - df_clean['YearRemodAdd']

# Has basement?
df_clean['HasBasement'] = df_clean['BsmtQual'].apply(lambda x: 0 if x == 'None' else 1)

# Has garage?
df_clean['HasGarage'] = df_clean['GarageType'].apply(lambda x: 0 if x == 'None' else 1)

# Has fireplace?
df_clean['HasFireplace'] = df_clean['FireplaceQu'].apply(lambda x: 0 if x == 'None' else 1)

print("\nNew features created:")
new_features = ['TotalSF', 'TotalBath', 'TotalPorchSF', 'HouseAge', 'RemodelAge',
                'HasBasement', 'HasGarage', 'HasFireplace']
for feat in new_features:
    print(f"  - {feat}")

# 6. HANDLING CATEGORICAL VARIABLES
print("\n" + "=" * 50)
print("6. Encoding Categorical Variables")
print("=" * 50)

# First, let's check categorical columns
cat_cols = df_clean.select_dtypes(include=['object']).columns
print(f"Number of categorical columns: {len(cat_cols)}")

# Ordinal features (have inherent order) - from data_description.txt
# Taking Ordinal features using AI
ordinal_features = {
    'ExterQual': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
    'ExterCond': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
    'BsmtQual': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
    'BsmtCond': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
    'BsmtExposure': {'None': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4},
    'BsmtFinType1': {'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6},
    'BsmtFinType2': {'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6},
    'HeatingQC': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
    'KitchenQual': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
    'FireplaceQu': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
    'GarageQual': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
    'GarageCond': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
    'PoolQC': {'None': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4},
    'Fence': {'None': 0, 'MnWw': 1, 'GdWo': 2, 'MnPrv': 3, 'GdPrv': 4}
}

print(f"\nOrdinal features to encode with mapping: {len(ordinal_features)}")

# Applying ordinal encoding
for col, mapping in ordinal_features.items():
    if col in df_clean.columns:
        df_clean[col] = df_clean[col].map(mapping).fillna(0)

# One-hot encode remaining categorical variables
nominal_cols = [col for col in cat_cols if col not in ordinal_features.keys()]
print(f"\nNominal features for one-hot encoding: {len(nominal_cols)}")

# one-hot encoding
df_encoded = pd.get_dummies(df_clean, columns=nominal_cols, drop_first=True)

print(f"\nShape after encoding: {df_encoded.shape}")
print(f"Original shape: {train_df.shape}")
print(f"Number of new features created: {df_encoded.shape[1] - train_df.shape[1]}")

# 7. SPLITTING DATA
print("\n" + "=" * 50)
print("7. Splitting Data for Training and Testing")
print("=" * 50)

# Separate features and target
# Dropping Id (not useful for prediction) and SalePrice (our target)
X = df_encoded.drop(['Id', 'SalePrice'], axis=1, errors='ignore')
y = df_encoded['SalePrice']

print(f"Features (X) shape: {X.shape}")
print(f"Target (y) shape: {y.shape}")

# Splitting into train and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, shuffle=True
)

print(f"\nTraining set: {X_train.shape[0]} samples")
print(f"Test set: {X_test.shape[0]} samples")
print(f"Split ratio: {X_train.shape[0]/(X_train.shape[0]+X_test.shape[0])*100:.1f}% train, " +
      f"{X_test.shape[0]/(X_train.shape[0]+X_test.shape[0])*100:.1f}% test")

# 8. SAVING PROCESSED DATA
print("\n" + "=" * 50)
print("8. Saving Processed Data")
print("=" * 50)

import pickle

# Saving the processed data
with open('processed_data_improved.pkl', 'wb') as f:
    pickle.dump((X_train, X_test, y_train, y_test), f)

# Saving feature names for reference
with open('feature_names.pkl', 'wb') as f:
    pickle.dump(list(X.columns), f)

print("✓ Processed data saved to 'processed_data_improved.pkl'")
print("✓ Feature names saved to 'feature_names.pkl'")

# 9. SUMMARY AND NEXT STEPS
print("\n" + "=" * 70)
print("PREPROCESSING COMPLETE - SUMMARY")
print("=" * 70)

print("\nWhat we accomplished:")
print("1. ✓ Loaded and understood data using data_description.txt")
print("2. ✓ Intelligently handled 'NA' values (they meant 'None', not missing)")
print("3. ✓ Created 8 new meaningful features based on domain knowledge")
print("4. ✓ Properly encoded ordinal categorical variables with correct order")
print("5. ✓ One-hot encoded nominal categorical variables")
print("6. ✓ Split data into training (80%) and testing (20%) sets")
print("7. ✓ Saved processed data for model training")

print(f"\nDataset sizes:")
print(f"  Original: {train_df.shape}")
print(f"  Processed: {df_encoded.shape}")
print(f"  Training set: {X_train.shape}")
print(f"  Test set: {X_test.shape}")